<!DOCTYPE html>
<html lang="en">
 <head>
  <link rel="stylesheet" href="/style.css" type="text/css">
  <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" type="text/css" href="/style.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üèïÔ∏è</text></svg>">
  <title></title>
 </head>
 <body>
  <div id="page-wrapper">
   <div id="header" role="banner">
    <header class="banner">
     <div id="banner-text">
      <span class="banner-title"><a href="/">beauhilton</a></span>
     </div>
    </header>
    <nav>
     <a href="/about">about</a>
<a href="/now">now</a>
<a class="nav-active" href="/posts">posts</a>
<a href="https://talks.beauhilton.com">talks</a>
<a href="/contact">contact</a>
<a href="/atom.xml">rss</a>
    </nav>
   </div>
   <main>
    <h1>
     good machine learning in medicine
    </h1>
    <p>
     <time id="post-date">2022-12-29</time>
    </p>
    <p id="post-excerpt">
     Because most machine learning in medicine sucks, I thought you might like to see an example of it done well.
    </p>
    <p>
     <a href="https://doi.org/10.1056/CAT.22.0214">https://doi.org/10.1056/CAT.22.0214</a>
    </p>
    <p>
     At least on the surface, this looks good. The article is paywalled so
I haven‚Äôt gotten to dig into their methods yet, but will update when I
am able to read the whole thing. The approach is the thing I‚Äôm keyed in
on - implementation details of course matter, but the biggest problem
with machine learning in medicine is not technique, but angle. Too
often, ML is a new shiny, no meat on its bones (I‚Äôm not discounting some
of the delightful advances in e.g. automatic read of ophthalmologic
images, that‚Äôs kickass tech and truly practice-changing, but it does
amount to image processing. The stuff I‚Äôm talking about here is not a
smarter camera, but automated gestalt handed to you by a mechanical
Turk).
    </p>
    <h2>
     ML is mostly for automation, but automating medicine is scary
    </h2>
    <p>
     The big idea is that while machine learning is great for many things,
the most important thing in industrial machine learning is teaching a
machine to make independent decisions to make things easier for humans
who already have too much cognitive load, or not enough hands, and a
pesky attachment to oxygen, food, and sleep. E.g. if there‚Äôs a knob
someone needs to turn when a certain complex set of things happens, and
sometimes the human forgets because they were occupied with all the
switches and buttons instead, or they were on lunch, or sleeping off the
Superbowl party, it sure would be nice to make an algorithm that can
independently assess whether the complex condition has been sufficiently
met, and go ahead and turn the knob for you (and maybe, while it‚Äôs at
it, get you a few ibuprofen for that hangover headache).
    </p>
    <p>
     In medicine we usually don‚Äôt want a bot making independent decisions,
but this paper from UPMC is a great example of the kind of independent
decision we could stomach, or even welcome.
    </p>
    <h2>
     build a model that is high-yield and low-risk
    </h2>
    <p>
     This system builds a mortality model, which is fun in itself, but
then goes to the next level to automate an e-consult to the palliative
care team for patients at highest risk of mortality after discharge.
    </p>
    <p>
     (Avati et al did the same basic thing at Stanford using a neural
network, which is a fine technology, but their explanatory models were
these shitty, ugly text-based tables that make you want to stab yourself
in the eyes - <a href="https://doi.org/10.1186%2Fs12911-018-0677-8">https://doi.org/10.1186%2Fs12911-018-0677-8</a>).
    </p>
    <p>
     It‚Äôs beautiful. Nothing bad bubbles up if the algorithm falls down
altogether (we‚Äôre just back where we were before the model went live).
Nothing horrible happens if the algorithm makes a weird claim (every
consult will still be triaged through a human). Possible positives are
numerous. The conversation itself may be one of the most pivotal in the
patient‚Äôs end-of-life journey, the hospital system will likely have
reduced readmissions for cases that should be managed at home with
hospice, and we will have more data to put towards figuring out how to
identify the modifiable risk factors for early post-discharge death.
    </p>
    <p>
     This team used the same tech I did when I built a mortality model for
CCF, my favorite kind of algorithm, tree-based models called
gradient-boosting machines (GBMs).
    </p>
    <h2>
     interpretability - gotta do it, maybe they did
    </h2>
    <p>
     What I can‚Äôt see yet is if they took the next obvious step, which is
to apply interpretability models on top. The main reason to use a GBM,
in my mind, other than they‚Äôre fast to train compared to neural networks
and perform the same if there‚Äôs enough data and you tune them properly,
is that they‚Äôre inherently compatible with the best meta-models that
allow you to interrogate, both on a per-prediction and cohort level, why
the model is saying whatever it‚Äôs saying - they‚Äôre actually less of a
black box than many standard statistical models, believe it or not.
    </p>
    <p>
     The best tool for doing this is called SHAP, and the output is
gorgeous - <a href="https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html">check
it out</a>. We used it here, I think to lovely effect: <a href="https://www.nature.com/articles/s41746-020-0249-z">https://www.nature.com/articles/s41746-020-0249-z</a>,
and it‚Äôs only gotten better since then)
    </p>
    <p>
     The other thing I love about pairing the interpretability models with
the predictive models is now you have something you can really learn
from, and, hence, teach from.
    </p>
    <h2>
     but, will this worsen burnout?
    </h2>
    <p>
     The main issue, (given that the model works and has been proven
trustworthy) and which I don‚Äôt think I‚Äôve heard anyone talk about in
great depth, is the new alert fatigue this kind of system would make,
for already overworked palliative care teams, and what mitigations they
are taking to keep the firehose of new possible consults manageable. One
thing we could do, and I have faith our house staff could do it well,
would be to implement the same system and have it first trigger an alert
to the primary team, with a recommendation to have the convo and reach
out to pall care if there is any hint of a loose end, or an automated
pivot to pall care if the notes don‚Äôt document GOC within a certain
number of days of the alert (or could pop up one of those boxes we love
so‚Ä¶ much with a radio button asking you if you‚Äôve had ‚Äúthe talk‚Äù
yet).
    </p>
    <p>
     Anywho, I‚Äôm not saying I want to actually do this project, I‚Äôve got
other stuff going on, but if you‚Äôre reading this you‚Äôre the kind of
person who is interested in what tech can do for a hospital system, and
this is a model (ha) combination of the very cutting edge of tech and
the oldest technique we have, which is to offer a hand to the dying and
with them face the abyss. My vision of the future is less Skynet, cold
and isolated, but rather humans forefront, with machines that run in the
background to nudge us into and help make room for more (non-digital,
non-Apple mediated) facetime.
    </p>
   </main>
   <div id="footnotes"></div>
   <footer></footer>
  </div>
 </body>
</html>
